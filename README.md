# rasberry_zero-ai
 run a tiny LLM on a Raspberry Pi Zero
Running a Tiny LLM on a $10 Raspberry Pi Zero!

After lots of tweaking, optimizations, and patience, I finally managed to run a tiny LLM on a Raspberry Pi Zero, a device that costs just $10! ðŸŽ‰

Running LLMs on ultra-low-power devices means AI can be embedded into IoT, edge security, and automation without relying on the cloud.

Even with 512MB RAM and a weak CPU, a quantized TinyLLM (SmolLM / TinyLlama) is able to generate responses locally.

This unlocks offline AI assistants, smart home automation, edge security, and more!

ðŸ”§ Optimizations used:

âœ” 4-bit quantization (GGUF format)

âœ” Llama.cpp for efficient inference

âœ” Swappiness tweaks to handle memory

ðŸ’¡ This proves that even the smallest devices can run AI!
